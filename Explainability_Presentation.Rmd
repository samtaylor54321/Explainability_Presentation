---
title: 'Through the Looking Glass: Explainability in Machine Learning'
author: "Sam Taylor"
date: "25/04/2019"
output:
  slidy_presentation: default
  ioslides_presentation: default
  beamer_presentation: default
---

 
```{r setup, include=FALSE, echo =FALSE, warning=FALSE}
knitr::opts_chunk$set(cache=TRUE)
#knitr::opts_chunk$set(echo = FALSE)
 
# Load Packages
 
library(tidyverse)
library(broom)
library(mlr)
library(evtree)
library(forcats)
library(randomForest)
library(caret)
library(mmpf)
library(lime)
library(iml)
library(reticulate)
library(rpart)
library(rpart.plot)

# Load Dataset
 
credit_raw <- evtree::GermanCredit
 
# Set theme to BW
 
theme_set(theme_bw())
 
# Preprocessing ---------------------------------------------------------------------
 
credit_processed <- credit_raw %>%
  transmute(good_loan = credit_risk,
            positive_account_balance = if_else(status =="... < 0 DM",0,1),
            loan_duration = duration,
            loan_purpose = fct_recode(as.factor(purpose),
                                      car = "car (new)",
                                      car = "car (used)",
                                      furniture = "furniture/equipment",
                                      electronics = "radio/television",
                                      other = "domestic appliances",
                                      other = "repairs",
                                      education = "education",
                                      education = "retraining",
                                      business = "business",
                                      other = "others"),
            loan_amount = amount,
            employment_length  = employment_duration,
            age = age
  )
 
# create dummy variables using reference category
 
credit_processed <- createDummyFeatures(credit_processed, target="good_loan", method='reference')
 
# create holdout set
 
trainIndex <- createDataPartition(credit_processed$good_loan, p = .6, list = FALSE, times = 1)
 
credit_train <- credit_processed[trainIndex, ]
credit_test <- credit_processed[-trainIndex, ]
 
# Set task
 
class_task <- makeClassifTask(data=credit_train, target="good_loan", positive='good')
 
# create glm learner
 
glm_learner <- makeLearner(cl="classif.logreg", predict.type ='prob')
 
# train glm learner on task
 
glm_model <- mlr::train(glm_learner, class_task)
 
# create random forest learner
 
rf_learner <- makeLearner(cl="classif.randomForest", predict.type ='prob')
 
# Train random forest learner on task
 
rf_model <- mlr::train(rf_learner, class_task)
 
```
 
Introduction
====================================
 
- Interpretability and why do we need it?
- Interpretable Models
- Permutation Importances
- Partial Dependancy Plots
- LIME
- Shapley Values
 
Interpretability & why we need it
====================================
 
- Regulation (GDPR)
- Ethical Obligations & Trust
- Debugging & Feature Engineering
- Informing Decision Making
 
![](images/accuracy_tradeoff.png)

Facebook implements explainability on Newsfeeds
====================================
 
![https://www.theguardian.com/technology/2019/apr/01/why-am-i-seeing-this-new-facebook-tool-to-demystify-news-feed](images/facebook.png)

 
The Problem
====================================
 
German Credit Dataset, identify good and bad loans based on a number of different variables including account balance, loan duration, loan amount, number of dependants, purpose of loan, age and if the loanee is employed.
 
1000 records split 2/3 training and 1/3 holdout test set.
 
Interpretable Models (Logistic Regression)
====================================
 
- Deploying interpretable models sacrificing performance but maximising explainability of the model.
- Coefficients of Logistic Regression represent the impact on the Log Odds based on a one unit change in the feature value
 
```{r logistic_regression, echo = FALSE, warning=FALSE}
 
# coefficientS
 
terms <- coef(getLearnerModel(glm_model, more.unwrap = TRUE))
values <- coef(getLearnerModel(glm_model, more.unwrap = TRUE)) %>% tibble()
 
tibble(terms = names(terms), values = values$.) %>% mutate(positive = values >0) %>%
  filter(terms !="(Intercept)") %>%
  ggplot(aes(x=fct_reorder(terms, values), y=values, fill = positive)) +
  geom_col(show.legend = FALSE) + coord_flip() + theme_bw() +
  ylab("Coefficients") + xlab("") +
  ggtitle("Logistic Regression Coefficients") +
  scale_fill_manual(values = c('#66A61E','#FF0000'))
 
```
 
Interpretable Models (Logistic Regression)
====================================
 
The modeling of the predictions as a weighted sum makes it transparent how predictions are produced. And with Lasso we can ensure that the number of features used remains small.
 
Many people use linear regression models. This means that in many places it is accepted for predictive modeling and doing inference. There is a high level of collective experience and expertise, including teaching materials on linear regression models and software implementations. Linear regression can be found in R, Python, Java, Julia, Scala, Javascript, …
 
Mathematically, it is straightforward to estimate the weights and you have a guarantee to find optimal weights (given all assumptions of the linear regression model are met by the data).
 
Together with the weights you get confidence intervals, tests, and solid statistical theory. There are also many extensions of the linear regression model (see chapter on GLM, GAM and more).
4.1.9 Disadvantages
 
Linear regression models can only represent linear relationships, i.e. a weighted sum of the input features. Each nonlinearity or interaction has to be hand-crafted and explicitly given to the model as an input feature.
 
Linear models are also often not that good regarding predictive performance, because the relationships that can be learned are so restricted and usually oversimplify how complex reality is.
 
The interpretation of a weight can be unintuitive because it depends on all other features. A feature with high positive correlation with the outcome y and another feature might get a negative weight in the linear model, because, given the other correlated feature, it is negatively correlated with y in the high-dimensional space. Completely correlated features make it even impossible to find a unique solution for the linear equation. An example: You have a model to predict the value of a house and have features like number of rooms and size of the house. House size and number of rooms are highly correlated: the bigger a house is, the more rooms it has. If you take both features into a linear model, it might happen, that the size of the house is the better predictor and gets a large positive weight. The number of rooms might end up getting a negative weight, because, given that a house has the same size, increasing the number of rooms could make it less valuable or the linear equation becomes less stable, when the correlation is too strong.
 
Interpretable Models (Decision Trees)
====================================
 
- Explanation (Quick Summary)
- Decision Trees are easy to explain to a human (if they are short!) with a straightforward visualisation.
- Bias/Variance Tradeoff
 
```{r, echo =FALSE}
 
decision_tree <- rpart(good_loan~., data = credit_train)
 
rpart.plot(decision_tree)
 
```
 
Permutation Importance
====================================
 
- Permutation importance is calculated after a model has been fitted. So we won't change the model or change what predictions we'd get for a given value of height, sock-count, etc.
 
- Instead we will ask the following question: If I randomly shuffle a single column of the validation data, leaving the target and all other columns in place, how would that affect the accuracy of predictions in that now-shuffled data?
 
```{r rf, echo=FALSE}
 
# Get Permutation Importances
 
importance <- generateFeatureImportanceData(class_task, method = 'permutation.importance',
                              learner =rf_learner, measure =auc)
 
# Plot Importance
 
importance$res %>% gather(feature, importance) %>%
  ggplot(aes(x=fct_reorder(feature, desc(importance)), y=abs(importance),fill =feature)) +
  geom_col(show.legend = FALSE) + coord_flip() + theme_bw() +
  labs(title = "Random Forest Permutation Importance") +
  xlab("") + ylab("Permutation Importance") + scale_fill_hue(h = c(200, 300))
```
 
Partial Dependance Plots
====================================
 
- Like permutation importance, partial dependence plots are calculated after a model has been fit.
 
- we repeatedly alter the value for one variable to make a series of predictions. We could predict the outcome if the team had the ball only 40% of the time. We then predict with them having the ball 50% of the time. Then predict again for 60%. And so on. We trace out predicted outcomes (on the vertical axis) as we move from small values of ball possession to large values (on the horizontal axis).
 
-  we plot the average predicted outcome on the vertical axis.
 
```{r PDP, echo = FALSE}
 
# Generate Partial Dependance Plot
 
pd_plots <- generatePartialDependenceData(rf_model, class_task)
 
tibble(percent_good = pd_plots$data$good,loan_duration = pd_plots$data$loan_duration) %>%
  filter(!is.na(loan_duration)) %>%
  ggplot(aes(x=loan_duration, y=percent_good)) + geom_line() + theme_bw() + geom_point() +
  xlab("Loan Duration") + ylab("Predicted Outcome (Good Loan)") +
  ggtitle("Partial Dependance Plot - Loan Duration")
 
```
 
LIME
====================================
 
Behind the workings of lime lies the (big) assumption that every complex model is linear on a local scale. While this is not justified in the paper it is not difficult to convince yourself that this is generally sound — you usually expect two very similar observations to behave predictably even in a complex model. lime then takes this assumption to its natural conclusion by asserting that it is possible to fit a simple model around a single observation that will mimic how the global model behaves at that locality. The simple model can then be used to explain the predictions of the more complex model locally.
 
The general approach lime takes to achieving this goal is as follows
 
- For each prediction to explain, permute the observation n times.
- Let the complex model predict the outcome of all permuted observations.
- Calculate the distance from all permutations to the original observation.
- Convert the distance to a similarity score.
- Select m features best describing the complex model outcome from the permuted data.
- Fit a simple model to the permuted data, explaining the complex model outcome with the m features from the permuted data weighted by    its similarity to the original observation.
- Extract the feature weights from the simple model and use these as explanations for the complex models local behavior.
 
It is clear from the above that there’s much wiggle-room in order to optimize the explanation. Chief among the choices that influence the quality of the explanation is how permutations are created, how permutation similarity is calculated, how, and how many, features are selected, and which model is used as the simple model. Some of these choices are hard-coded into lime, while others can be influenced by the user.
 
LIME
====================================
 
LIME Explanation of a single prediction using LASSO.
 
```{r LIME, echo=FALSE}
 
# Build Caret Model for ease
 
caret_model <- caret::train(credit_train[, -1], credit_train$good_loan, method = 'rf')
 
# Create an explainer object
 
explainer <- lime(credit_train[, -1], caret_model)
 
# Explain new observation
 
explanation <- explain(credit_test[, -1], explainer, n_labels = 1, n_features = 5, feature_select ='lasso_path')
 
# Plot Explanations
 
single_explanation <- explanation %>% filter(case ==2 )
 
plot_features(single_explanation)
 
```
 
LIME
====================================
 
- The underlying machine learning model used for prediction can change however the interpretable model can be retained throughout. It    also allows for statements to be made about changes in predictions.
 
- Explanations can be made sparse through shrinkage methods (LASSO) or short decision trees which make them much more straightforward    to interpret. It is possible to use other features than the original model which are less abstract (eg. text embeddings vs presence    of words in sentences)
 
- It may not necessarily be sufficient for full explanations which may not be practical for GDPR requirements or debugging any models    produced.
 
- LIME's fidelity measure provides an estimate of how good that particular model is and if it can be considered reliable.
 
- Defining the neighbourhood
 
- Replicability of explanations?
 
I want to play a game...
====================================
 
Explanation - average marginal contribution across all possible coalitions. It takes the form of a null player if it isn't in the coalition (how much does it differ if this feature takes
a base value rather than actual values). SHAP is slightly different to Shapley Values to avoid it becoming computationally intractable (2k) but the premise is the same.
the premise is the same.
 
The Shapley value can be misinterpreted. The Shapley value of a feature value is not the difference of the predicted value after removing the feature from the model training. The interpretation of the Shapley value is: Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value.
 
Shapley Values (Single Prediction)
====================================

![](images/Screenshot 2019-04-24 at 12.10.34.png)
<!-- import numpy as np -->
<!-- import pandas as pd -->
<!-- from sklearn.model_selection import train_test_split -->
<!-- from sklearn.ensemble import RandomForestClassifier -->
<!-- import shap  -->
<!-- import matplotlib -->

<!-- data = pd.read_csv("http://freakonometrics.free.fr/german_credit.csv") -->
<!-- y = data['Creditability']  -->
<!-- data=data.drop(['Creditability'], axis=1) -->
<!-- feature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]] -->
<!-- X = data[feature_names] -->
<!-- train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1) -->
<!-- my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y) -->

<!-- row_to_show = 5 -->
<!-- data_for_prediction = val_X.iloc[row_to_show]   -->

<!-- # use 1 row of data here. Could use multiple rows if desired -->

<!-- data_for_prediction_array = data_for_prediction.values.reshape(1, -1) -->

<!-- my_model.predict_proba(data_for_prediction_array) -->

<!-- # Create object that can calculate shap values -->
<!-- explainer = shap.TreeExplainer(my_model) -->

<!-- # Calculate Shap values -->
<!-- shap_values = explainer.shap_values(data_for_prediction) -->

<!-- # Plot SHAP Value -->

<!-- shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction,matplotlib=True) -->

 
Shapley Values (Multiple Predictions)
====================================
 
![](images/Screenshot 2019-04-24 at 12.11.37.png)
<!-- import numpy as np -->
<!-- import pandas as pd -->
<!-- from sklearn.model_selection import train_test_split -->
<!-- from sklearn.ensemble import RandomForestClassifier -->
<!-- import shap  -->
<!-- import matplotlib -->

<!-- shap.initjs() -->
<!-- data = pd.read_csv("http://freakonometrics.free.fr/german_credit.csv") -->
<!-- y = data['Creditability']  -->
<!-- X=data.drop(['Creditability'], axis=1) -->

<!-- train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1) -->
<!-- my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y) -->

<!-- # Create object that can calculate shap values -->
<!-- explainer = shap.TreeExplainer(my_model) -->

<!-- # calculate shap values. This is what we will plot. -->
<!-- # Calculate shap_values for all of val_X rather than a single row, to have more data for plot. -->
<!-- shap_values = explainer.shap_values(val_X) -->

<!-- # Make plot. Index of [1] is explained in text below. -->
<!-- shap.force_plot(explainer.expected_value[0], shap_values[1], val_X, link ='logit') -->

Shapley Values (Summary Plots)
====================================

![](images/Screenshot 2019-04-24 at 12.10.20.png)
<!-- import numpy as np -->
<!-- import pandas as pd -->
<!-- from sklearn.model_selection import train_test_split -->
<!-- from sklearn.ensemble import RandomForestClassifier -->
<!-- import shap  -->
<!-- import matplotlib -->

<!-- shap.initjs() -->
<!-- data = pd.read_csv("http://freakonometrics.free.fr/german_credit.csv") -->
<!-- y = data['Creditability']  -->
<!-- X=data.drop(['Creditability'], axis=1) -->

<!-- train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1) -->
<!-- my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y) -->

<!-- # Create object that can calculate shap values -->
<!-- explainer = shap.TreeExplainer(my_model) -->

<!-- # calculate shap values. This is what we will plot. -->
<!-- # Calculate shap_values for all of val_X rather than a single row, to have more data for plot. -->
<!-- shap_values = explainer.shap_values(val_X) -->

<!-- # Make plot. Index of [1] is explained in text below. -->
<!-- shap.summary_plot(shap_values[0], val_X) -->


Shapley Values
====================================
 
- The difference between the prediction and the average prediction is fairly distributed among the feature values. This property distinguishes the Shapley value from other methods such as LIME. LIME does not guarantee that the prediction is fairly distributed among the features.
 
- The Shapley value allows contrastive explanations. Instead of comparing a prediction to the average prediction of the entire dataset, you could compare it to a subset or even to a single data point. This contrastiveness is also something that local models like LIME do not have.
 
- The Shapley value requires a lot of computing time. In 99.9% of real-world problems, only the approximate solution is feasible. An exact computation of the Shapley value is computationally expensive because there are 2k possible coalitions of the feature values. Sampling coalitions may assist here.
 
- The Shapley value is the wrong explanation method if you seek sparse explanations. Explanations created with the Shapley value method always use all the features.
 
- The Shapley value returns a simple value per feature, but no prediction model like LIME. This means it cannot be used to make statements about changes in prediction for changes in the input, such as: “If I were to earn €300 more a year, my credit score would increase by 5 points.”
 
Squaring the Circle
====================================
 
- Permutation Importances => SHAP
- Issues with compute (takes approx 7700 hours to work for entire case load) - TrainExplainer
- KMeans Clustering prior to reduce the number of explanations required.
- NLP
- "Unhelpful Features"
- Making explanations sparse by excluding certain features which fall below a specific threshold.
 
{Example Plot}
 
References
====================================
 
Molnar, C. (2019), "Interpretable Machine Learning",
https://christophm.github.io/interpretable-ml-book/
 
Alvarez-Melis, David, and Tommi S. Jaakkola. (2018),
“On the robustness of interpretability methods”
arXiv preprint arXiv:1806.08049
 
Shapley, Lloyd S. (1953) “A value for n-person games.”,
Contributions to the Theory of Games 2.28: 307-317
 