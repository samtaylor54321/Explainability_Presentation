---
title: 'Through the Looking Glass: Explainability in Machine Learning'
author: "Sam Taylor"
date: "09/10/2019"
output:
  slidy_presentation: default
  powerpoint_presentation: default

---

```{r setup, include=FALSE, echo =FALSE, warning=FALSE}
knitr::opts_chunk$set(cache=TRUE)
#knitr::opts_chunk$set(echo = FALSE)
 
# Load Packages
 
library(tidyverse)
library(broom)
library(mlr)
library(evtree)
library(forcats)
library(randomForest)
library(caret)
library(mmpf)
library(lime)
library(iml)
library(reticulate)
library(rpart)
library(rpart.plot)

# Load Dataset
 
credit_raw <- evtree::GermanCredit
 
# Set theme to BW
 
theme_set(theme_bw())
 
# Preprocessing ---------------------------------------------------------------------
 
credit_processed <- credit_raw %>%
  transmute(good_loan = credit_risk,
            positive_account_balance = if_else(status =="... < 0 DM",0,1),
            loan_duration = duration,
            loan_purpose = fct_recode(as.factor(purpose),
                                      car = "car (new)",
                                      car = "car (used)",
                                      furniture = "furniture/equipment",
                                      electronics = "radio/television",
                                      other = "domestic appliances",
                                      other = "repairs",
                                      education = "education",
                                      education = "retraining",
                                      business = "business",
                                      other = "others"),
            loan_amount = amount,
            employment_length  = employment_duration,
            age = age
  )
 
# create dummy variables using reference category
 
credit_processed <- createDummyFeatures(credit_processed, target="good_loan", method='reference')
 
# create holdout set
 
trainIndex <- createDataPartition(credit_processed$good_loan, p = .6, list = FALSE, times = 1)
 
credit_train <- credit_processed[trainIndex, ]
credit_test <- credit_processed[-trainIndex, ]
 
# Set task
 
class_task <- makeClassifTask(data=credit_train, target="good_loan", positive='good')
 
# create glm learner
 
glm_learner <- makeLearner(cl="classif.logreg", predict.type ='prob')
 
# train glm learner on task
 
glm_model <- mlr::train(glm_learner, class_task)
 
# create random forest learner
 
rf_learner <- makeLearner(cl="classif.randomForest", predict.type ='prob')
 
# Train random forest learner on task
 
rf_model <- mlr::train(rf_learner, class_task)
 
```
 
Introduction
====================================

- Interpretability and why do we need it?
- Interpretable Models
- Permutation Importances
- Partial Dependancy Plots
- LIME
- Shapley Values
- Use Case
 
Interpretability & why we need it
====================================
 
- Regulation (GDPR)
- Ethical Obligations & Trust
- Debugging & Feature Engineering
- Informing Decision Making
 
<center> 
![](images/accuracy_tradeoff.png)
</center>

Facebook implements explainability on Newsfeeds
====================================

<center>  
![https://www.theguardian.com/technology/2019/apr/01/why-am-i-seeing-this-new-facebook-tool-to-demystify-news-feed](images/facebook.png)
</center>

Interpretable Models (Logistic Regression)
====================================
 
- Deploying interpretable models sacrificing performance but maximising explainability of the model, especially in cases of non-linear data.

- Coefficients of Logistic Regression represent the impact on the Log Odds based on a one unit change in the feature value
 
```{r logistic_regression, echo = FALSE, warning=FALSE}
 
# coefficientS
 
terms <- coef(getLearnerModel(glm_model, more.unwrap = TRUE))
values <- coef(getLearnerModel(glm_model, more.unwrap = TRUE)) %>% tibble()
 
tibble(terms = names(terms), values = values$.) %>% mutate(positive = values >0) %>%
  filter(terms !="(Intercept)") %>%
  ggplot(aes(x=fct_reorder(terms, values), y=values, fill = positive)) +
  geom_col(show.legend = FALSE) + coord_flip() + theme_bw() +
  ylab("Coefficients") + xlab("") +
  ggtitle("Logistic Regression Coefficients") +
  scale_fill_manual(values = c('#66A61E','#FF0000'))
 
```
 
Interpretable Models (Decision Trees)
====================================
 
- Tree based models split the data multiple times based a loss function (eg. Gini)

- Decision Trees are easy to explain to a human (if they are short!) with a straightforward visualisation.

- Bias/Variance Tradeoff.
 
```{r, echo =FALSE}
decision_tree <- rpart(good_loan~., data = credit_train)
 
rpart.plot(decision_tree)
```
 
Permutation Importance
====================================
 
- We measure the importance of a feature by calculating the increase in the model’s prediction error after permuting the feature.

- A feature is “important” if shuffling its values increases the model error, because in this case the model relied on the feature for the prediction. 

- A feature is “unimportant” if shuffling its values leaves the model error unchanged, because in this case the model ignored the feature for the prediction. 

```{r rf, echo=FALSE}
 
# Get Permutation Importances
 
importance <- generateFeatureImportanceData(class_task, method = 'permutation.importance', learner =rf_learner, measure =auc)
 
# Plot Importance
 
importance$res %>% gather(feature, importance) %>%
  ggplot(aes(x=fct_reorder(feature, desc(importance)), y=abs(importance),fill =feature)) +
  geom_col(show.legend = FALSE) + coord_flip() + theme_bw() +
  labs(title = "Random Forest Permutation Importance") +
  xlab("") + ylab("Permutation Importance") + scale_fill_hue(h = c(200, 300))
```
 
Partial Dependance Plots
====================================
 
- "The partial dependence plot shows the marginal effect one or two features have on the predicted outcome of a machine learning model" (J. H. Friedman 2001). 
 
- Repeatedly altering the value for one variable to make a series of predictions, plotting predicted outcomes on the vertical axis and variable of interest on on the horizontal axis.
 
```{r PDP, echo = FALSE}
 
# Generate Partial Dependance Plot
 
pd_plots <- generatePartialDependenceData(rf_model, class_task)
 
tibble(percent_good = pd_plots$data$good,loan_duration = pd_plots$data$loan_duration) %>%
  filter(!is.na(loan_duration)) %>%
  ggplot(aes(x=loan_duration, y=percent_good)) + geom_line() + theme_bw() + geom_point() +
  xlab("Loan Duration") + ylab("Predicted Outcome (Good Loan)") +
  ggtitle("Partial Dependance Plot - Loan Duration")
 
```
 
LIME
====================================
 
- Assumption is that every complex model is linear on a local scale. It is possible to fit a simple model around a single observation that will mimic how the global model behaves at that locality. 
 
<center>   
![](images/lime.png)
</center>

1. Select your instance of interest for which you want to have an explanation of
   its black box prediction.
2. Perturb your dataset and get the black box predictions for these new points.
3. Weight the new samples according to their proximity to the instance of interest.
4. Train a weighted, interpretable model on the dataset with the variations.
5. Explain the prediction by interpreting the local model.

LIME
====================================
 
- LIME Explanation of a single prediction using LASSO as local interpretable model.
 
```{r LIME, echo=FALSE}
 
# Build Caret Model for ease
 
caret_model <- caret::train(credit_train[, -1], credit_train$good_loan, method = 'rf')
 
# Create an explainer object
 
explainer <- lime(credit_train[, -1], caret_model)
 
# Explain new observation
 
explanation <- explain(credit_test[, -1], explainer, n_labels = 1, n_features = 5, feature_select ='lasso_path')
 
# Plot Explanations
 
single_explanation <- explanation[1:3, ]
 
plot_features(single_explanation)
 
```
 
LIME
====================================
 
- The underlying machine learning model used for prediction can change however the interpretable model can be retained throughout.

- It also allows for statements to be made about changes in predictions.
 
- Explanations can be made sparse through shrinkage methods (LASSO) or 
  short decision trees 
 
- It may not necessarily be sufficient for full explanations which may not be practical for GDPR requirements or debugging any models produced.
 
- LIME's fidelity measure provides an estimate of how good that particular model is and if it can be considered reliable.
 
- Many optimizers to be tuned!
 
- Replicability of explanations?

I want to play a game...
====================================

<center>
![](images/shapley.png)
</center>

The Shapley value of a feature value is its contribution to the payout, weighted and summed over all possible feature value combinations. Where S is a subset of the features used in the model, x is the vector of feature values of the instance to be explained and p the number of features.

1. Efficiency - The feature contributions must add up to the difference of prediction for x and the average. 

2. Symmetry - The contributions of two feature values j and k should be the same if they contribute equally to all possible coalitions.

3. Dummy - A feature j that does not change the predicted value – regardless of which coalition of feature values it is added to – should have a Shapley value of 0.

4. Additivity - For a game with combined payouts val+val+ the respective Shapley values are as follows:

Shapley Values - Single Prediction
====================================
<center> 
![](images/Screenshot 2019-04-24 at 12.10.34.png)
</center> 

Shapley Values - All Predictions
====================================
 
<center>  
![](images/Screenshot 2019-04-24 at 12.11.37.png)
</center> 

Shapley Values - Summary Plots
====================================

<center> 
![](images/Screenshot 2019-04-24 at 12.10.20.png)
</center> 

Shapley Values - Here be Dragons!
====================================
 
- Differs from LIME in that the difference between the prediction and the average prediction is fairly distributed among the feature values. 
 
- Computing time (2k possible coalitions) - sampling coalitions may assist here.
 
- Shapley values provide complete explanations (not sparse!) and will always use all the features.
 
- No predictive model; it cannot be used to make statements about changes in prediction for changes in the input.
 
Squaring the Circle
====================================
 
- Permutation Importances => SHAP
- Issues with compute (takes approx 7700 hours to work for entire case load) -   TreeExplainer
- KMeans Clustering prior to reduce the number of explanations required.
- NLP
- "Unhelpful Features"
- Making explanations sparse by excluding certain features which fall below a    specific threshold.

References
====================================
 
Molnar, C. (2019), "Interpretable Machine Learning",
https://christophm.github.io/interpretable-ml-book/
 
Alvarez-Melis, David, and Tommi S. Jaakkola. (2018),
“On the robustness of interpretability methods”
arXiv preprint arXiv:1806.08049
 
Shapley, Lloyd S. (1953) “A value for n-person games.”,
Contributions to the Theory of Games 2.28: 307-317

Ribeiro, M. Singh, S., Guestrin, C. (2016)
"Why Should I Trust You?: Explaining the Predictions of Any Classifier" 


 