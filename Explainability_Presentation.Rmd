---
title: 'Through the Looking Glass: Interpretability in Machine Learning'
author: 'Sam Taylor,  Github: @samtaylor54321'
date: "10/04/2019"
output:
  slidy_presentation: default
  powerpoint_presentation: default
---

```{r setup, include=FALSE, echo =FALSE}
knitr::opts_chunk$set(cache=TRUE)
#knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(broom)
library(mlr)

url="http://freakonometrics.free.fr/german_credit.csv"
credit_raw <- read.csv(url, header = TRUE, sep = ",")

# Preprocessing ---------------------------------------------------------------------

credit_processed <- credit_raw %>% 
              transmute(good_loan = Creditability,
                     positive_account_balance = if_else(Account.Balance ==1,0,1), 
                     loan_duration = Duration.of.Credit..month.,
                     loan_purpose = fct_recode(as.factor(Purpose), 
                                                 car = "0",
                                                 car = "1",
                                                 furniture = "2",
                                                 electronics = "3",
                                                 other = "4",
                                                 other = "5",
                                                 education = "6",
                                                 education = "8",
                                                 business = "9",
                                                 other = "10"),
                     loan_amount = Credit.Amount,
                     employed  = if_else( Length.of.current.employment == 1, 0, 1),
                     dependents = No.of.dependents,
                     contact_details_provided = if_else(Telephone ==1, 0 , 1),
                     age = Age..years.
                  )

# create dummy variables using reference category

credit_processed <- createDummyFeatures(credit_processed, target = "good_loan", method = "reference")

# Set task

class_task <- makeClassifTask(data=credit_processed, target="good_loan")

# create holdout set

hold_out <- makeResampleInstance("Holdout",class_task)

credit_train <- subsetTask(class_task,hold_out$train.inds[[1]]) 
credit_test <- subsetTask(class_task,hold_out$test.inds[[1]])

# create training task

training_task <- makeClassifTask(data=credit_train$env$data, target="good_loan")

```

Introduction 
====================================

- Interpretability and why do we need it? 
- Interpretable Models
- Permutation Importances
- Partial Dependancy Plots
- Model Agnostic Approaches (LIME & SHAP)
- Intelligent Triage

Interpretability & why we need it
====================================
- Regulation (GDPR)
- Ethical Obligations & Trust
- Debugging & Feature Engineering
- Informing Decision Making

![](images/accuracy_tradeoff.png)

Facebook implements explainability on Newsfeeds
====================================

![https://www.techapeek.com/2019/04/01/facebook-to-show-explanations-why-content-shows-up-on-users-news-feeds/](images/facebook.png) 

Intelligent Triage
====================================

DWP needs to provide a welfare system to the population of the UK, but needs to decide which customers are cheating the system and which aren't. How do use machine learning to do this and be able to explain to our customers why this is the case. 

A toy example - German Credit Dataset, identify good and bad loans based on a number of different variables including account balance, loan duration, loan amount, number of dependants, purpose of loan, age and if the loanee is employed. 

1000 records split 2/3 training and 1/3 holdout test set. 

Approach 1: Interpretable Models
====================================

Deploying interpretable models, sacrificing performance but maximising explainability of the model. 

Coefficients of Logistic Regression

```{r cars, echo = FALSE}

# Set learner

glm_learner <- makeLearner(cl="classif.logreg", predict.type ='prob')

# Train learner on task

glm_model <- train(glm_learner, training_task)

# coefficients

coef(getLearnerModel(glm_model, more.unwrap = TRUE)) #%>% knitr::kable()

# Assess model performance

glm_pred <- predict(object=glm_model,newdata=credit_test$env$data)

```

Performance of Logistic Regression Model

```{r, echo = FALSE}
performance(glm_pred, measures = list(f1, auc))
```

Approach 2: Permutation Importance
====================================

- Permutation importance is calculated after a model has been fitted. So we won't change the model or change what predictions we'd get for a given value of height, sock-count, etc.

- Instead we will ask the following question: If I randomly shuffle a single column of the validation data, leaving the target and all other columns in place, how would that affect the accuracy of predictions in that now-shuffled data?

```{r, echo=FALSE}
# Set learner

rf_learner <- makeLearner(cl="classif.ranger",
                          predict.type ='prob')

# Train learner on task

rf_model <- train(rf_learner, class_task)

# Get Permutation Importances

importance <- generateFeatureImportanceData(class_task, method = 'permutation.importance',
                              learner =rf_learner, measure =f1)
```

Need to change colour pallet

```{r, echo=FALSE}
importance$res %>% gather(feature, importance) %>% 
  ggplot(aes(x=fct_reorder(feature, desc(importance)), y=abs(importance),fill =feature)) + 
  geom_col(show.legend = FALSE) + coord_flip() + theme_bw() +
  labs(title = "Random Forest Permutation Importance") +
  xlab("") + ylab("Permutation Importance")
```

Assess model performance

```{r, echo=FALSE}
rf_pred <- predict(object=rf_model,newdata=credit_test$env$data)

performance(rf_pred, measures = list(f1, auc))
```

Approach 3: Partial Dependance Plots
====================================

- Like permutation importance, partial dependence plots are calculated after a model has been fit.

- we repeatedly alter the value for one variable to make a series of predictions. We could predict the outcome if the team had the ball only 40% of the time. We then predict with them having the ball 50% of the time. Then predict again for 60%. And so on. We trace out predicted outcomes (on the vertical axis) as we move from small values of ball possession to large values (on the horizontal axis).

-  we plot the average predicted outcome on the vertical axis.

{Plot}

Approach 4: LIME
====================================

Explanation - fitting a local interpretible model to the feature  space in order to 
explain a specific prediction. 
Code
Graphic
Pros and Cons


Approach 5: SHAP 
====================================

Explanation - average marginal contribution across all possible coalitions. It takes the form of a null player if it isn't in the coalition (how much does it differ if this feature takes 
a base value rather than actual values). SHAP is slightly different to Shapley Values to avoid it becoming computationally intractable (2k) but the premise is the same. 
the premise is the same.

Example that works for video as well. Can it be applied to audio data????
Code (Python/R Code)
Graphic (Shapley Plots)
Pros and Cons 

```{python}
x = 42 * 2
print(x) 
```

Use Case
====================================

Intelligent Triage
Example Plot
NLP

